{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a custom pipeline\n",
    "\n",
    "GPJax does not support sharing parameters across mean and kernel. Therefore, a custom model must be defined (in lieu of the traditional posterior) as well as a custom objective function (which is a modified `gpx.objectives.ConjugateMLL`) to perform a fit (obtain optimised posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gpjax_wp289/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import gpjax as gpx\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from gpjax.base import param_field\n",
    "from dataclasses import dataclass, field\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "from gpjax.distributions import GaussianDistribution\n",
    "\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "from jaxtyping import (\n",
    "    Float,\n",
    "    Num,\n",
    ")\n",
    "\n",
    "import cola\n",
    "from cola.ops import (\n",
    "    Dense,\n",
    "    LinearOperator,\n",
    ")\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "abs_path = os.path.abspath('../src')\n",
    "sys.path.append(abs_path)\n",
    "from p53_data import JAXP53_Data, dataset_3d, generate_test_times\n",
    "from kernels import latent_kernel\n",
    "from custom_gps import p53_posterior\n",
    "from plotter import plot_gp\n",
    "\n",
    "from gpjax.dataset import Dataset\n",
    "\n",
    "import beartype.typing as tp\n",
    "Kernel = tp.TypeVar(\"Kernel\", bound=\"gpjax.kernels.base.AbstractKernel\")\n",
    "CustomModel = tp.TypeVar(\"Model\", bound=\"p53_model\")\n",
    "\n",
    "import optax as ox\n",
    "key = jr.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "@dataclass\n",
    "class p53_model(gpx.base.Module):\n",
    "    \"\"\"\n",
    "    Implementation of p53 model from Lawrence et al. 2006\n",
    "    \"\"\"\n",
    "    ####### Define parameters\n",
    "\n",
    "    # Sensitivities of the genes\n",
    "    initial_sensitivities = jnp.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=jnp.float64)\n",
    "\n",
    "    true_s: Float[Array, \"1 5\"] = param_field(\n",
    "        initial_sensitivities,\n",
    "        bijector=tfb.Softplus(),\n",
    "        metadata={\"name\": \" kxx_sensitivities\"},\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Degradation rates of the genes\n",
    "    initial_decays = jnp.array([0.4, 0.4, 0.4, 0.4, 0.4], dtype=jnp.float64)\n",
    "\n",
    "    true_d: Float[Array, \"1 5\"] = param_field(\n",
    "        initial_decays,\n",
    "        bijector=tfb.Softplus(),\n",
    "        metadata={\"name\": \" kxx_degradations\"},\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Sigmoid to map real numbers to (0,1) and then scales and shifts to get(0.5, 3.5) # NOTE: tfb.Chain acts Right to Left\n",
    "    l_bijector = tfb.Chain(\n",
    "        [\n",
    "            tfb.Shift(jnp.array(0.5, dtype=jnp.float64)),\n",
    "            tfb.Scale(jnp.array(3.0, dtype=jnp.float64)),\n",
    "            tfb.Sigmoid(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    initial_lengthscale = jnp.array(2.5, dtype=jnp.float64)\n",
    "\n",
    "    l: Float[Array, \" O\"] = param_field(\n",
    "        initial_lengthscale, bijector=l_bijector, metadata={\"name\": \"lengthscale\"}\n",
    "    )\n",
    "    \n",
    "    initial_constrained_b = jnp.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=jnp.float64)\n",
    "    \n",
    "    true_b: Float[Array, \"1 5\"] = param_field(initial_constrained_b, bijector=tfb.Softplus(), metadata={\"name\": \" basal_rates\"}, trainable=True)\n",
    "\n",
    "    \n",
    "    # Define mean function (TODO: change zero mean)\n",
    "    def mean_function(self, x: Num[Array, \"N D\"]) -> Float[Array, \"N O\"]:\n",
    "        return jnp.ones((x.shape[0], 1)) * 0\n",
    "    \n",
    "    # Define kernel\n",
    "    def kernel(self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \n",
    "        # Get flag from input (1 = gene expression, 0 = latent force function)\n",
    "        f1 = jnp.array(t[2], dtype=int)\n",
    "        f2 = jnp.array(t_prime[2], dtype=int)\n",
    "\n",
    "        # Cannot use if statements in kernels -> use switches\n",
    "        kxx_switch = f1 * f2\n",
    "        kff_switch = (1 - f1) * (1 - f2)\n",
    "        kxf_switch = f1 * (1 - f2)\n",
    "        kxf_t_switch = (1 - f1) * f2\n",
    "\n",
    "        final_kernel = (\n",
    "            kxx_switch * self.kernel_xx(t, t_prime)\n",
    "            + kff_switch * self.kernel_ff(t, t_prime)\n",
    "            + kxf_switch * self.kernel_xf(t, t_prime)\n",
    "            + kxf_t_switch * self.kernel_xf(t, t_prime).T\n",
    "        )\n",
    "\n",
    "        return final_kernel\n",
    "    \n",
    "    def kernel_xx(self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \n",
    "        # Get gene indices\n",
    "        j = t[1].astype(int)\n",
    "        k = t_prime[1].astype(int)\n",
    "\n",
    "        t = t[0]\n",
    "        t_prime = t_prime[0]\n",
    "\n",
    "        # Equation 5\n",
    "        mult = self.true_s[j] * self.true_s[k] * self.l * jnp.sqrt(jnp.pi) * 0.5\n",
    "        second_term = self.h(k, j, t_prime, t) + self.h(j, k, t, t_prime)\n",
    "\n",
    "        kxx = mult * second_term\n",
    "        \n",
    "        return kxx.squeeze()\n",
    "    \n",
    "    def kernel_xf(self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \n",
    "        gene_xpr = jnp.where(t[2] == 0, t_prime, t)\n",
    "        latent_force = jnp.where(t[2] == 0, t, t_prime)\n",
    "\n",
    "        j = gene_xpr[1].astype(int)\n",
    "\n",
    "        # Slice inputs\n",
    "        gene_xpr = gene_xpr[0]\n",
    "        latent_force = latent_force[0]\n",
    "\n",
    "        t_dist = gene_xpr - latent_force\n",
    "\n",
    "        first_term = 0.5 * self.l * jnp.sqrt(jnp.pi) * self.true_s[j]\n",
    "        first_expon_term = jnp.exp(self.gamma(j) ** 2)\n",
    "        second_expon_term = jnp.exp(-self.true_d[j] * t_dist)\n",
    "        erf_terms = jax.scipy.special.erf(\n",
    "            (t_dist / self.l) - self.gamma(j)\n",
    "        ) + jax.scipy.special.erf(latent_force / self.l + self.gamma(j))\n",
    "\n",
    "        kxf = first_term * first_expon_term * second_expon_term * erf_terms\n",
    "        \n",
    "        return kxf.squeeze()\n",
    "    \n",
    "    def kernel_ff(self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]) -> ScalarFloat:\n",
    "            \n",
    "        t = t[0].reshape(-1)\n",
    "        t_prime = t_prime[0].reshape(-1)\n",
    "\n",
    "        sq_dist = jnp.square(t.reshape(-1, 1) - t_prime)\n",
    "        sq_dist = jnp.divide(sq_dist, 2 * self.l.reshape((-1, 1)))\n",
    "\n",
    "        K = jnp.exp(-sq_dist)\n",
    "\n",
    "        return K.squeeze() \n",
    "    \n",
    "    # Helper functions\n",
    "    def h(self, j: Float[Array, \" O\"], k: Float[Array, \" O\"], t1: Float[Array, \" O\"], t2: Float[Array, \" O\"]) -> ScalarFloat:\n",
    "\n",
    "        t_dist = t2 - t1\n",
    "\n",
    "        multiplier = jnp.exp(self.gamma(k) ** 2) / (self.true_d[j] + self.true_d[k])\n",
    "\n",
    "        first_multiplier = jnp.exp(-self.true_d[k] * t_dist)\n",
    "        \n",
    "        first_erf_terms = jax.scipy.special.erf(\n",
    "            (t_dist / self.l) - self.gamma(k)\n",
    "        ) + jax.scipy.special.erf(t1 / self.l + self.gamma(k))\n",
    "\n",
    "        second_multiplier = jnp.exp(-(self.true_d[k] * t2 + self.true_d[j] * t1))\n",
    "        \n",
    "        second_erf_terms = jax.scipy.special.erf(\n",
    "            (t2 / self.l) - self.gamma(k)\n",
    "        ) + jax.scipy.special.erf(self.gamma(k))\n",
    "\n",
    "        result = multiplier * (\n",
    "            jnp.multiply(first_multiplier, first_erf_terms)\n",
    "            - jnp.multiply(second_multiplier, second_erf_terms)\n",
    "        )\n",
    "\n",
    "        #print(f\"result: {result}\")\n",
    "        return result\n",
    "\n",
    "    def gamma(self, k: Float[Array, \" O\"]) -> ScalarFloat:\n",
    "        # Gamma term for h function\n",
    "        return (self.true_d[k] * self.l) / 2\n",
    "    \n",
    "    \n",
    "    # Calculate cross-covariance\n",
    "    def cross_covariance(\n",
    "        self, kernel: Kernel, x: Float[Array, \"N D\"], y: Float[Array, \"M D\"]\n",
    "    ) -> Float[Array, \"N M\"]:\n",
    "        cross_cov = jax.vmap(lambda x: jax.vmap(lambda y: kernel(x, y))(y))(x)\n",
    "        return cross_cov\n",
    "\n",
    "    def gram(self, kernel: Kernel, x: Float[Array, \"N D\"]) -> LinearOperator:\n",
    "        Kxx = self.cross_covariance(kernel, x, x)\n",
    "        return cola.PSD(Dense(Kxx))\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # Define predict method\n",
    "    ############################\n",
    "    def latent_predict(\n",
    "        self, test_inputs: Num[Array, \"N D\"], train_data: JAXP53_Data\n",
    "    ) -> GaussianDistribution:\n",
    "    \n",
    "        x, y, variances = dataset_3d(train_data)\n",
    "        t = test_inputs\n",
    "        jitter = 1e-4\n",
    "\n",
    "        diag_variances = jnp.diag(variances.reshape(-1))\n",
    "        Kxx = self.gram(self.kernel, x)\n",
    "        Kxx += diag_variances\n",
    "        Kxx += cola.ops.I_like(Kxx) * jitter\n",
    "        K_inv = cola.inv(Kxx)\n",
    "\n",
    "        Kxf = self.cross_covariance(self.kernel, x, t)\n",
    "        KfxKxx = jnp.matmul(Kxf.T, K_inv.to_dense())\n",
    "        mean = jnp.matmul(KfxKxx, y)\n",
    "\n",
    "        Kff = self.gram(self.kernel, t)\n",
    "        Kff += cola.ops.I_like(Kff) * jitter * 10\n",
    "\n",
    "        var = Kff - jnp.matmul(KfxKxx, Kxf)\n",
    "        var = jnp.diag(jnp.diag(var.to_dense()))\n",
    "        var += cola.ops.I_like(var) * jitter * 10\n",
    "\n",
    "        return GaussianDistribution(jnp.atleast_1d(mean.squeeze()), var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConjMLL(gpx.objectives.AbstractObjective):\n",
    "    def step(self, model: CustomModel, train_data: Dataset) -> ScalarFloat:\n",
    "        x, y = train_data.X, train_data.y\n",
    "\n",
    "        obs_noise = jnp.array([1.0e-3], dtype=jnp.float64)**2\n",
    "        mx = model.mean_function(x)\n",
    "\n",
    "        # Σ = (Kxx + Io²) = LLᵀ\n",
    "        Kxx = model.gram(model.kernel, x)\n",
    "        Kxx += cola.ops.I_like(Kxx) * 1e-4\n",
    "        Sigma = Kxx + cola.ops.I_like(Kxx) * obs_noise\n",
    "        Sigma = cola.PSD(Sigma)\n",
    "\n",
    "        # p(y | x, θ), where θ are the model hyperparameters:\n",
    "        mll = GaussianDistribution(jnp.atleast_1d(mx.squeeze()), Sigma)\n",
    "\n",
    "        return self.constant * (mll.log_prob(jnp.atleast_1d(y.squeeze())).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p53_data = JAXP53_Data(replicate=0, data_dir='../data')\n",
    "training_times, gene_expressions, variances = dataset_3d(p53_data)\n",
    "\n",
    "dataset_train = gpx.Dataset(training_times, gene_expressions)\n",
    "\n",
    "testing_times = generate_test_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = gpx.Dataset(training_times, gene_expressions)\n",
    "custom_posterior = p53_model()\n",
    "loss = CustomConjMLL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Running: 100%|██████████| 100/100 [00:01<00:00, 67.17it/s, Value=-369969.09]\n"
     ]
    }
   ],
   "source": [
    "trained_model, history = gpx.fit(model=custom_posterior, objective=loss, train_data=dataset_train, optim=ox.adam(0.01), key=key, num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p53_model(true_s=Array([0.5079606 , 0.76891069, 0.47335069, 1.69672681, 0.49294833],      dtype=float64), true_d=Array([0.79575179, 0.95452123, 0.85041765, 0.17260294, 0.82241917],      dtype=float64), l=Array(3.1002091, dtype=float64), true_b=Array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=float64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpjax_wp289",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
