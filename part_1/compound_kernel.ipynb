{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Kernel for latent forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpjax as gpx\n",
    "import os\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.random as jr\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from beartype.typing import Union\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "from gpjax.base import param_field, static_field\n",
    "from gpjax.kernels.base import AbstractKernel\n",
    "from gpjax.kernels.stationary.utils import squared_distance\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "\n",
    "from beartype.typing import Callable\n",
    "from jaxtyping import Int\n",
    "\n",
    "from p53_dataset import JAXP53_Data, load_barenco_data, flatten_dataset_jax\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\")\n",
    "\n",
    "# Check if LaTeX is in notebook path\n",
    "if os.environ.get('PATH') is not None:\n",
    "    if 'TeX' not in os.environ['PATH']:\n",
    "        os.environ['PATH'] += os.pathsep + '/Library/TeX/texbin'\n",
    "\n",
    "colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basal transcription rate (B), transcript degradation rate (D), sensitivity of gene (S)\n",
    "def params_ground_truth():\n",
    "    B_exact = np.array([0.0649, 0.0069, 0.0181, 0.0033, 0.0869])\n",
    "    S_exact = np.array([0.9075, 0.9748, 0.9785, 1.0000, 0.9680])\n",
    "    D_exact = np.array([0.2829, 0.3720, 0.3617, 0.8000, 0.3573])\n",
    "    return B_exact, S_exact, D_exact\n",
    "\n",
    "# Define transcription rates measured by Barenco et al. (plotted points on GP graph)\n",
    "f_observed = np.array([0.1845,1.1785,1.6160,0.8156,0.6862,-0.1828, 0.5131])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_dir = os.path.join(os.getcwd(), '..', 'data')\n",
    "(original_genes_df, genes_transformed), (tfs_df, tfs_transformed), gene_var, tf_var, times = load_barenco_data(data_dir)\n",
    "\n",
    "p53_data = JAXP53_Data(replicate=3)\n",
    "train_t , train_y = flatten_dataset_jax(p53_data)\n",
    "\n",
    "# Set random key\n",
    "key = jr.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to data class itself (?)\n",
    "\n",
    "num_genes = p53_data.num_outputs\n",
    "# (num_genes, dimension, num_timepoints)\n",
    "gene_data = jnp.array([p53_data[i] for i in range(num_genes)])\n",
    "\n",
    "time_points = gene_data[0,0,:]\n",
    "time_points_repeated = jnp.repeat(time_points, gene_data.shape[0])\n",
    "gene_indices = jnp.repeat(jnp.arange(gene_data.shape[0]), len(time_points))\n",
    "\n",
    "\n",
    "# Shape (t x j) x 3 where t is timepoints and j is genes\n",
    "training_times = jnp.stack((time_points_repeated, gene_indices, jnp.ones(num_genes * len(time_points), dtype=int)), axis=-1)\n",
    "\n",
    "\n",
    "# Shape (t x j) x 1 where t is timepoints and j is genes\n",
    "gene_expressions = gene_data[:,1,:].flatten().reshape(-1,1)\n",
    "\n",
    "dataset_train = gpx.Dataset(training_times, gene_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test points (t, i, 1):\n",
    "\n",
    "def gen_test_times(t=100):\n",
    "    \"\"\"\n",
    "    Generate testing times for the model of shape (N, 3) where N is the number of testing times. Default is N = 100\n",
    "    \"\"\"\n",
    "    times = jnp.linspace(0, 12, t)\n",
    "    # Gene indices shouldn't matter\n",
    "    gene_indices = jnp.repeat(-1, t)\n",
    "    testing_times = jnp.stack((times, gene_indices, jnp.repeat(0,t)), axis=-1)\n",
    "    return testing_times\n",
    "\n",
    "gen_test_times().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_times = gen_test_times()\n",
    "\n",
    "train_row = dataset_train.X[20]\n",
    "test_row = testing_times[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef __call__(self, X: \\'Float[Array, \"1 D\"]\\', Xp: \\'Float[Array, \"1 D\"]\\') -> \\'Float[Array, \"1\"]\\':\\n    z = jnp.array(X[2], dtype=int)\\n    zp = jnp.array(Xp[2], dtype=int)\\n\\n    # Switches for different kernel functions\\n    kxx_switch = (1 - z) * (1 - zp)\\n    kff_switch = z * zp\\n    kxf_switch = (1 - z) * zp\\n    kxf_t_switch = z * (1 - zp)\\n\\n    # Compute the kernel value based on the switches\\n    result = (kxx_switch * self.kxx(X, Xp) +\\n                kff_switch * self.kff(X, Xp) +\\n                kxf_switch * self.kxf(X, Xp) +\\n                kxf_t_switch * self.kxf_transpose(X, Xp))\\n    return result\\n        \\n'"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def __call__(self, X: 'Float[Array, \"1 D\"]', Xp: 'Float[Array, \"1 D\"]') -> 'Float[Array, \"1\"]':\n",
    "    z = jnp.array(X[2], dtype=int)\n",
    "    zp = jnp.array(Xp[2], dtype=int)\n",
    "\n",
    "    # Switches for different kernel functions\n",
    "    kxx_switch = (1 - z) * (1 - zp)\n",
    "    kff_switch = z * zp\n",
    "    kxf_switch = (1 - z) * zp\n",
    "    kxf_t_switch = z * (1 - zp)\n",
    "\n",
    "    # Compute the kernel value based on the switches\n",
    "    result = (kxx_switch * self.kxx(X, Xp) +\n",
    "                kff_switch * self.kff(X, Xp) +\n",
    "                kxf_switch * self.kxf(X, Xp) +\n",
    "                kxf_t_switch * self.kxf_transpose(X, Xp))\n",
    "    return result\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel is element wise (i.e. rows)\n",
    "def kernel_switch_testing(X, Y):\n",
    "    \"\"\"\n",
    "    Testing switch implementation for kernel switching.\n",
    "\n",
    "    Parameters:\n",
    "    X: Row of input with shape (1, 3)\n",
    "    Y: Row of input withshape (1, 3)\n",
    "\n",
    "    Returns:\n",
    "        Integer\n",
    "    \"\"\"\n",
    " \n",
    "    f1 = jnp.array(X[2], dtype=int)\n",
    "    f2 = jnp.array(Y[2], dtype=int)\n",
    "    \n",
    "\n",
    "    kxx_switch = f1 * f2\n",
    "    kff_switch = (1 - f1) * (1 - f2)\n",
    "    kxf_switch = f1 * (1 - f2)\n",
    "    kxf_t_switch = (1 - f1) * f2\n",
    "\n",
    "    print(f'kxx: {kxx_switch}')\n",
    "    print(f'kff: {kff_switch}')\n",
    "    print(f'kxf: {kxf_switch}')\n",
    "    print(f'kfx: {kxf_t_switch}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPJAX KERNEL IS ELEMENT WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.54132485, dtype=float32)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_constraint = tfb.Softplus()\n",
    "\n",
    "init_s = pos_constraint.inverse(1.0)\n",
    "init_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called\n",
      "H called\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class test_kernel(gpx.kernels.AbstractKernel):\n",
    "\n",
    "\n",
    "    def __call__(self, x: Float[Array, \" D\"], y: Float[Array, \" D\"]) -> ScalarFloat:\n",
    "\n",
    "        print('called')\n",
    "\n",
    "\n",
    "        h = self.h()\n",
    "        print(h)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def h(self):\n",
    "        print('H called')\n",
    "        return 1\n",
    "    \n",
    "test_ker = test_kernel()\n",
    "test_ker(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class p53_kxx(gpx.kernels.AbstractKernel):\n",
    "    r\"Covariance function of gene expressions j and k at time t and t'\"\n",
    "\n",
    "    # TODO: review inverse initialisation method - check if init as list or single param\n",
    "    pos_constraint: Callable = tfb.Softplus()\n",
    "    initial_s: Float[Array, \" O\"] = static_field(pos_constraint.inverse(1.0))\n",
    "    initial_d: Float[Array, \" O\"] = static_field(pos_constraint.inverse(0.4))\n",
    "    \n",
    "    # Use dummy variables before trying inverse method\n",
    "    initial_s_test: Float[Array, \" O\"] = static_field(jnp.array([1.0]))\n",
    "    initial_d_test: Float[Array, \" O\"] = static_field(jnp.array([1.0]))\n",
    "\n",
    "    # Define sensitivities for both genes\n",
    "    s_j: Float[Array, \" O\"] = param_field(initial_s_test, bijector=pos_constraint)\n",
    "    s_k: Float[Array, \" O\"] = param_field(initial_s_test, bijector=pos_constraint)\n",
    "\n",
    "    # Define decay rates for both genes\n",
    "    d_j: Float[Array, \" O\"] = param_field(initial_d_test, bijector=pos_constraint)\n",
    "    d_k: Float[Array, \" O\"] = param_field(initial_d_test, bijector=pos_constraint)\n",
    "\n",
    "    # Use measured sensiivities and decays at first\n",
    "    true_s: Float[Array, \"1 5\"] = static_field(jnp.array([0.9075, 0.9748, 0.9785, 1.0000, 0.9680]))\n",
    "    true_d: Float[Array, \"1 5\"] = static_field(jnp.array([0.2829, 0.3720, 0.3617, 0.8000, 0.3573]))\n",
    "\n",
    "    # Use dummy lengthscale at first\n",
    "    l: Float[Array, \" O\"] = static_field(jnp.array([2.5]))\n",
    "    \n",
    "    \n",
    "    def __call__(self, t: Int[Array, \"1 3\"], t_prime: Int[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \"\"\"\n",
    "        Equation 5 in paper k_xx(t,t')\n",
    "        \"\"\"\n",
    "\n",
    "        # Error trap (JAX friendly)\n",
    "        def check_validity(condition):\n",
    "            if condition:\n",
    "                raise ValueError(\"t or t' cannot be testing points (z=0)\")\n",
    "\n",
    "        condition = jnp.logical_or(t[2] == 0, t_prime[2] == 0)\n",
    "        jax.debug.callback(check_validity, condition)\n",
    "        \n",
    "        # Get gene indices\n",
    "        j = t[1].astype(int)\n",
    "        k = t_prime[1].astype(int)\n",
    "\n",
    "        \"\"\"\n",
    "        print('kxx has been called')\n",
    "        print(self.true_s)\n",
    "        print(self.true_d)\n",
    "        print(t.shape)\n",
    "        print(t_prime.shape)\n",
    "\n",
    "        print(f'lengthscale {self.l}, {type(self.l)}')\n",
    "        \"\"\"\n",
    "\n",
    "        # NOTE: using index yields same results\n",
    "        t = self.slice_input(t)\n",
    "        t_prime = self.slice_input(t_prime)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        print('Inputs have been sliced')\n",
    "        print(t, t.shape)\n",
    "        print(t, t_prime.shape)\n",
    "        \"\"\"\n",
    "\n",
    "        mult = self.true_s[j] * self.true_s[k] * self.l * jnp.sqrt(jnp.pi) * 0.5\n",
    "        second_term = self.h(k,j, t_prime, t) + self.h(j,k, t, t_prime)\n",
    "        \n",
    "\n",
    "        kxx = mult * second_term\n",
    "\n",
    "        \"\"\"\n",
    "        print(second_term.shape)\n",
    "        print(f'mult {mult}, shape: {mult.shape}')\n",
    "        print(f'second term {second_term}, shape: {second_term.shape}')\n",
    "\n",
    "        print(f'kxx shape {kxx.shape}')\n",
    "        \"\"\"\n",
    "\n",
    "        return kxx.squeeze()\n",
    "        \n",
    "    \n",
    "    def h(self,j: Int[Array, \" O\"],k: Int[Array, \" O\"],t1: Int[Array, \" O\"],t2: Int[Array, \" O\"]) -> ScalarFloat:\n",
    "        \"\"\"\n",
    "        Analytical solution for the convolution of the exponential kernel with a step function.\n",
    "        \"\"\"\n",
    "\n",
    "        t_dist = jnp.abs(t1-t2)\n",
    "\n",
    "        multiplier = jnp.exp(self.gamma(k)**2) / (self.true_d[j] + self.true_d[k])\n",
    "\n",
    "        first_multiplier = jnp.exp(-self.true_d[k]*t_dist)\n",
    "        first_erf_terms = jax.scipy.special.erf((t_dist / self.l) - self.gamma(k)) + jax.scipy.special.erf(self.gamma(k))\n",
    "\n",
    "        second_multiplier = jnp.exp(-(self.true_d[k]*t2 + self.true_d[j]*t1))\n",
    "        second_erf_terms = jax.scipy.special.erf((t2 / self.l) - self.gamma(k)) + jax.scipy.special.erf(self.gamma(k))\n",
    "\n",
    "        result = multiplier * (jnp.multiply(first_multiplier, first_erf_terms) - jnp.multiply(second_multiplier, second_erf_terms))\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def gamma(self, k: Int[Array, \" O\"]) -> ScalarFloat:\n",
    "        # Gamma term for h function\n",
    "        return (self.true_d[k] * self.l) /2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class p53_kxf(gpx.kernels.AbstractKernel):\n",
    "    r\"Cross-covariance between gene expression j at time t and f at time t'\"\n",
    "\n",
    "    # TODO: look at initialisation methods and sharing of parameters between covariances\n",
    "\n",
    "    # Use measured sensiivities and decays at first\n",
    "    true_s: Float[Array, \"1 5\"] = static_field(jnp.array([0.9075, 0.9748, 0.9785, 1.0000, 0.9680]))\n",
    "    true_d: Float[Array, \"1 5\"] = static_field(jnp.array([0.2829, 0.3720, 0.3617, 0.8000, 0.3573]))\n",
    "\n",
    "    # Use dummy lengthscale at first\n",
    "    l: Float[Array, \" O\"] = static_field(jnp.array([2.5]))\n",
    "\n",
    "    def __call__(self, t: Int[Array, \"1 3\"], t_prime: Int[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \n",
    "        # Error trap (JAX friendly)\n",
    "        def check_val_test(condition):\n",
    "            if condition:\n",
    "                raise ValueError(\"t and t' cannot both be testing points (z=0)\")\n",
    "            \n",
    "        def check_val_train(condition):\n",
    "            if condition:\n",
    "                raise ValueError(\"t and t' cannot both be training points (z=1)\")\n",
    "            \n",
    "        cond_test = jnp.logical_and(t[2] == 0, t_prime[2] == 0)\n",
    "        cond_train = jnp.logical_and(t[2] == 1, t_prime[2] == 1)\n",
    "\n",
    "        jax.debug.callback(check_val_test, cond_test)\n",
    "        jax.debug.callback(check_val_train, cond_train)\n",
    "\n",
    "        # Get gene expression and latent force from flag (kxf anf kfx are transposes)\n",
    "        gene_xpr = jnp.where(t[2] == 0, t_prime, t)\n",
    "        latent_force = jnp.where(t[2] == 0, t, t_prime)\n",
    "    \n",
    "        j = gene_xpr[1]\n",
    "\n",
    "        print(f'Latent force {latent_force}')\n",
    "        print(f'Gene expression {gene_xpr}')    \n",
    "        \n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class p53_combined_kernel(gpx.kernels.AbstractKernel):\n",
    "    r\"Combined covariance for learning the latent replication of p53\"\n",
    "\n",
    "    name: str = 'p53 Cov'\n",
    "\n",
    "    # Define external kernels\n",
    "    kernel_xx: gpx.kernels.AbstractKernel = field(default_factory=lambda: p53_kxx(active_dims=[0]))\n",
    "\n",
    "    kernel_xf: int = 2\n",
    "    kernel_ff: int = 3\n",
    "\n",
    "    def __call__(self, t: Int[Array, \"1 3\"], t_prime: Int[Array, \"1 3\"]) -> ScalarFloat:\n",
    "\n",
    "        # Get flag from input (1 = gene expression, 0 = latent force function)\n",
    "        f1 = jnp.array(t[2], dtype=int)\n",
    "        f2 = jnp.array(t_prime[2], dtype=int)\n",
    "\n",
    "        # Cannot use if statements in kernels -> use switches\n",
    "        kxx_switch = f1 * f2\n",
    "        kff_switch = (1 - f1) * (1 - f2)\n",
    "        kxf_switch = f1 * (1 - f2)\n",
    "        kxf_t_switch = (1 - f1) * f2\n",
    "\n",
    "        #kernel_to_use = kxx_switch * self.kernel_xx(t, t_prime) + kff_switch * self.kernel_ff(t, t_prime) + kxf_switch * self.kernel_xf(t, t_prime) + kxf_t_switch * self.kernel_xf(t_prime, t)\n",
    "\n",
    "        # .squeeze() ?\n",
    "        #return kernel_to_use\n",
    "        print('hi')\n",
    "        #kernel_kxx = jnp.array(self.kernel_xx(t, t_prime))\n",
    "        kernel_kxx = self.kernel_xx(t, t_prime)\n",
    "        print(kernel_kxx)\n",
    "  \n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at shared training parameters (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "-0.032596097197167516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p53_ker = p53_combined_kernel()\n",
    "p53_ker(train_row,train_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([8., 2., 1.], dtype=float64)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent force [ 2.42424242 -1.          0.        ]\n",
      "Gene expression [8. 2. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_xf = p53_kxf(active_dims=[0])\n",
    "k_xf(dataset_train.X[20],test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([8., 2., 1.], dtype=float64)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3)"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kxx_ker = p53_kxx(active_dims=[0])\n",
    "t1 = dataset_train.X\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs have been sliced\n",
      "[8.] (1,)\n",
      "[8.] (1,)\n",
      "8.0 ()\n",
      "8.0 ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(-0.0325961, dtype=float64)"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kxx_ker(train_row,train_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(kxx_ker.gram(t1).to_dense())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO PLOT KERNEL AND CHECK IT LOOKS OK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "noise = 0.3\n",
    "x_gpr = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f_gpr = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "\n",
    "kernel_gpr = gpx.kernels.RBF(active_dims=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.15513296,  0.        ],\n",
       "       [ 0.20999486,  0.        ],\n",
       "       [ 2.70871393,  0.        ],\n",
       "       [-0.24233199,  0.        ],\n",
       "       [ 0.62798972,  0.        ],\n",
       "       [-1.04605396,  0.        ],\n",
       "       [-0.29715902,  0.        ],\n",
       "       [ 0.89566759,  0.        ],\n",
       "       [-2.13326535,  0.        ],\n",
       "       [-1.59715151,  0.        ],\n",
       "       [ 0.02428609,  0.        ],\n",
       "       [ 1.92848615,  0.        ],\n",
       "       [ 1.6970173 ,  0.        ],\n",
       "       [-0.6035951 ,  0.        ],\n",
       "       [ 0.26813325,  0.        ],\n",
       "       [-0.25503812,  0.        ],\n",
       "       [ 2.57459184,  0.        ],\n",
       "       [-0.99307462,  0.        ],\n",
       "       [ 2.10230002,  0.        ],\n",
       "       [-1.58137953,  0.        ],\n",
       "       [ 1.57460624,  0.        ],\n",
       "       [ 0.12240851,  0.        ],\n",
       "       [-0.70544628,  0.        ],\n",
       "       [-1.86591987,  0.        ],\n",
       "       [-1.53468077,  0.        ],\n",
       "       [ 2.15706123,  0.        ],\n",
       "       [-1.70874549,  0.        ],\n",
       "       [-2.28593996,  0.        ],\n",
       "       [-0.66266154,  0.        ],\n",
       "       [ 0.61508803,  0.        ],\n",
       "       [ 2.17109311,  0.        ],\n",
       "       [ 0.2357363 ,  0.        ],\n",
       "       [ 1.01281358,  0.        ],\n",
       "       [ 0.2641702 ,  0.        ],\n",
       "       [ 2.81608552,  0.        ],\n",
       "       [ 1.65937594,  0.        ],\n",
       "       [ 0.90771755,  0.        ],\n",
       "       [ 0.575871  ,  0.        ],\n",
       "       [ 2.59903443,  0.        ],\n",
       "       [-0.10699546,  0.        ],\n",
       "       [-2.35179917,  0.        ],\n",
       "       [-2.08384883,  0.        ],\n",
       "       [-2.19956605,  0.        ],\n",
       "       [ 2.87842957,  0.        ],\n",
       "       [-1.59577327,  0.        ],\n",
       "       [-2.50404514,  0.        ],\n",
       "       [-2.93501159,  0.        ],\n",
       "       [-2.90492061,  0.        ],\n",
       "       [ 1.70862188,  0.        ],\n",
       "       [ 1.36790482,  0.        ],\n",
       "       [-1.4300857 ,  0.        ],\n",
       "       [-0.5245788 ,  0.        ],\n",
       "       [-0.43667227,  0.        ],\n",
       "       [-0.38118769,  0.        ],\n",
       "       [-0.79853644,  0.        ],\n",
       "       [-2.5947292 ,  0.        ],\n",
       "       [-0.77687526,  0.        ],\n",
       "       [-1.67737244,  0.        ],\n",
       "       [ 1.33946702,  0.        ],\n",
       "       [ 1.02414182,  0.        ],\n",
       "       [-1.3868861 ,  0.        ],\n",
       "       [ 1.37968158,  0.        ],\n",
       "       [-1.6194458 ,  0.        ],\n",
       "       [ 0.84857268,  0.        ],\n",
       "       [ 1.56640187,  0.        ],\n",
       "       [-0.41070465,  0.        ],\n",
       "       [-1.00654599,  0.        ],\n",
       "       [ 1.85781139,  0.        ],\n",
       "       [-1.81370437,  0.        ],\n",
       "       [-0.68089136,  0.        ],\n",
       "       [ 2.83650369,  0.        ],\n",
       "       [-0.55140034,  0.        ],\n",
       "       [-1.78634044,  0.        ],\n",
       "       [ 2.19685378,  0.        ],\n",
       "       [ 1.80444834,  0.        ],\n",
       "       [-1.11542926,  0.        ],\n",
       "       [ 2.69786122,  0.        ],\n",
       "       [-0.72118866,  0.        ],\n",
       "       [ 0.39203414,  0.        ],\n",
       "       [-1.37921512,  0.        ],\n",
       "       [-1.37504227,  0.        ],\n",
       "       [-1.23681397,  0.        ],\n",
       "       [ 0.14469467,  0.        ],\n",
       "       [-1.12270135,  0.        ],\n",
       "       [-0.33614141,  0.        ],\n",
       "       [-2.64589151,  0.        ],\n",
       "       [ 1.59492601,  0.        ],\n",
       "       [-1.10561629,  0.        ],\n",
       "       [ 2.42740041,  0.        ],\n",
       "       [ 0.62877862,  0.        ],\n",
       "       [-0.56280794,  0.        ],\n",
       "       [ 2.66521249,  0.        ],\n",
       "       [-0.13236756,  0.        ],\n",
       "       [ 1.7954025 ,  0.        ],\n",
       "       [-2.0609105 ,  0.        ],\n",
       "       [-1.19542842,  0.        ],\n",
       "       [-2.06575465,  0.        ],\n",
       "       [-0.34454231,  0.        ],\n",
       "       [ 0.14723158,  0.        ],\n",
       "       [-1.3484917 ,  0.        ]], dtype=float64)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpr_new = jnp.stack((x_gpr.reshape(-1,), jnp.repeat(0, 100)), axis=-1)\n",
    "x_gpr_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float64)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_gpr(x_gpr_new,x_gpr_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kxx: 0\n",
      "kff: 0\n",
      "kxf: 1\n",
      "kfx: 0\n"
     ]
    }
   ],
   "source": [
    "kernel_switch_testing(train_row, test_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(kxx_ker.gram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_wp289",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
