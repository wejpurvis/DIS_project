{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior distribution has no Stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpjax as gpx\n",
    "import os\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.random as jr\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from beartype.typing import Union\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "from gpjax.base import param_field, static_field\n",
    "from gpjax.kernels.base import AbstractKernel\n",
    "from gpjax.kernels.stationary.utils import squared_distance\n",
    "from gpjax.typing import (\n",
    "    Array,\n",
    "    ScalarFloat,\n",
    ")\n",
    "\n",
    "from beartype.typing import Callable\n",
    "from jaxtyping import Int\n",
    "\n",
    "from p53_data import JAXP53_Data, dataset_3d, generate_test_times\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "plt.style.use(\"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\")\n",
    "\n",
    "# Check if LaTeX is in notebook path\n",
    "if os.environ.get('PATH') is not None:\n",
    "    if 'TeX' not in os.environ['PATH']:\n",
    "        os.environ['PATH'] += os.pathsep + '/Library/TeX/texbin'\n",
    "\n",
    "colors = rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "key = jr.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p53_data = JAXP53_Data(replicate=0)\n",
    "training_times, gene_expressions, variances = dataset_3d(p53_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class latent_kernel(gpx.kernels.AbstractKernel):\n",
    "    r\"Combined covariance for learning the latent replication of p53\"\n",
    "\n",
    "    name: str = \"p53 Cov\"\n",
    "\n",
    "    # Define parameters\n",
    "    # BUG: crashes when using unconstrained parameters\n",
    "\n",
    "    # Sensitivities of the genes\n",
    "    initial_sensitivities = jnp.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=jnp.float64)\n",
    "    #initial_unconstrained_s = tfb.Softplus().inverse(initial_constrained_s)\n",
    "\n",
    "    true_s: Float[Array, \"1 5\"] = param_field(\n",
    "        initial_sensitivities,\n",
    "        bijector=tfb.Softplus(),\n",
    "        metadata={\"name\": \" kxx_sensitivities\"},\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Degradation rates of the genes\n",
    "    initial_decays = jnp.array([0.4, 0.4, 0.4, 0.4, 0.4], dtype=jnp.float64)\n",
    "    #initial_unconstrained_d = tfb.Softplus().inverse(initial_constrained_d)\n",
    "\n",
    "    true_d: Float[Array, \"1 5\"] = param_field(\n",
    "        initial_decays,\n",
    "        bijector=tfb.Softplus(),\n",
    "        metadata={\"name\": \" kxx_degradations\"},\n",
    "        trainable=True,\n",
    "    )\n",
    "\n",
    "    # Sigmoid to map real numbers to (0,1) and then scales and shifts to get(0.5, 3.5) # NOTE: tfb.Chain acts Right to Left\n",
    "    l_bijector = tfb.Chain(\n",
    "        [\n",
    "            tfb.Shift(jnp.array(0.5, dtype=jnp.float64)),\n",
    "            tfb.Scale(jnp.array(3.0, dtype=jnp.float64)),\n",
    "            tfb.Sigmoid(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    initial_lengthscale = jnp.array(2.5, dtype=jnp.float64)\n",
    "    #initial_unconstrained_l = l_bijector.inverse(initial_lengthscale)\n",
    "\n",
    "    l: Float[Array, \" O\"] = param_field(\n",
    "        initial_lengthscale, bijector=l_bijector, metadata={\"name\": \"lengthscale\"}\n",
    "    )\n",
    "\n",
    "    # Redefine parameters as static fields\n",
    "    #true_s: Float[Array, \"1 5\"] = static_field(jnp.array([1.0,1.0,1.0,1.0,1.0]))\n",
    "    #true_d: Float[Array, \"1 5\"] = static_field(jnp.array([0.4,0.4,0.4,0.4,0.4]))\n",
    "    #l: Float[Array, \" O\"] = static_field(jnp.array(2.5))\n",
    "\n",
    "    def __call__(self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]) -> ScalarFloat:\n",
    "        \n",
    "        # Get flag from input (1 = gene expression, 0 = latent force function)\n",
    "        f1 = jnp.array(t[2], dtype=int)\n",
    "        f2 = jnp.array(t_prime[2], dtype=int)\n",
    "\n",
    "        # Cannot use if statements in kernels -> use switches\n",
    "        kxx_switch = f1 * f2\n",
    "        kff_switch = (1 - f1) * (1 - f2)\n",
    "        kxf_switch = f1 * (1 - f2)\n",
    "        kxf_t_switch = (1 - f1) * f2\n",
    "\n",
    "        final_kernel = (\n",
    "            kxx_switch * self.kernel_xx(t, t_prime)\n",
    "            + kff_switch * self.kernel_ff(t, t_prime)\n",
    "            + kxf_switch * self.kernel_xf(t, t_prime)\n",
    "            + kxf_t_switch * self.kernel_xf(t_prime, t)\n",
    "        )\n",
    "\n",
    "        return final_kernel\n",
    "\n",
    "    def kernel_xx(\n",
    "        self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]\n",
    "    ) -> ScalarFloat:\n",
    "        \"\"\"\n",
    "        Equation 5 in paper k_xx(t,t')\n",
    "        \n",
    "        print('Input to kxx')\n",
    "        print(f\"t: {t}\")\n",
    "        print(f\"t_prime: {t_prime}\")\n",
    "        \"\"\"\n",
    "\n",
    "        # Error trap (JAX friendly)\n",
    "        def check_validity(condition):\n",
    "            if condition:\n",
    "                # raise ValueError(\"t or t' cannot be testing points (z=0)\")\n",
    "                return 0\n",
    "\n",
    "        condition = jnp.logical_or(t[2] == 0, t_prime[2] == 0)\n",
    "        jax.debug.callback(check_validity, condition)\n",
    "\n",
    "        # Get gene indices\n",
    "        j = t[1].astype(int)\n",
    "        k = t_prime[1].astype(int)\n",
    "\n",
    "        t = t[0]\n",
    "        t_prime = t_prime[0]\n",
    "\n",
    "        # Equation 5\n",
    "        mult = self.true_s[j] * self.true_s[k] * self.l * jnp.sqrt(jnp.pi) * 0.5\n",
    "        second_term = self.h(k, j, t_prime, t) + self.h(j, k, t, t_prime)\n",
    "\n",
    "        kxx = mult * second_term\n",
    "        \n",
    "        return kxx.squeeze()\n",
    "\n",
    "    def kernel_xf(\n",
    "        self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]\n",
    "    ) -> ScalarFloat:\n",
    "        # Get gene expression and latent force from flag (kxf anf kfx are transposes)\n",
    "        gene_xpr = jnp.where(t[2] == 0, t_prime, t)\n",
    "        latent_force = jnp.where(t[2] == 0, t, t_prime)\n",
    "\n",
    "        j = gene_xpr[1].astype(int)\n",
    "\n",
    "        # Slice inputs\n",
    "        gene_xpr = gene_xpr[0]\n",
    "        latent_force = latent_force[0]\n",
    "\n",
    "        t_dist = jnp.abs(gene_xpr - latent_force)\n",
    "\n",
    "        first_term = 0.5 * self.l * jnp.sqrt(jnp.pi) * self.true_s[j]\n",
    "        first_expon_term = jnp.exp(self.gamma(j) ** 2)\n",
    "        second_expon_term = jnp.exp(-self.true_d[j] * t_dist)\n",
    "        erf_terms = jax.scipy.special.erf(\n",
    "            (t_dist / self.l) - self.gamma(j)\n",
    "        ) + jax.scipy.special.erf(latent_force / self.l + self.gamma(j))\n",
    "\n",
    "        kxf = first_term * first_expon_term * second_expon_term * erf_terms\n",
    "        return kxf.squeeze()\n",
    "\n",
    "    def kernel_ff(\n",
    "        self, t: Float[Array, \"1 3\"], t_prime: Float[Array, \"1 3\"]\n",
    "    ) -> ScalarFloat:\n",
    "\n",
    "        t = t[0] / self.l\n",
    "        t_prime = t_prime[0] / self.l\n",
    "\n",
    "        K = jnp.exp(-0.5 * squared_distance(t, t_prime))\n",
    "\n",
    "        return K.squeeze()\n",
    "\n",
    "    # Helper functions\n",
    "    def h(\n",
    "        self,\n",
    "        j: Int[Array, \" O\"],\n",
    "        k: Int[Array, \" O\"],\n",
    "        t1: Float[Array, \" O\"],\n",
    "        t2: Float[Array, \" O\"],\n",
    "    ) -> ScalarFloat:\n",
    "        \"\"\"\n",
    "        Analytical solution for the convolution of the exponential kernel with a step function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Debug print\n",
    "        \"\"\"\n",
    "        print(f\"j: {j}\")\n",
    "        print(f\"k: {k}\")\n",
    "        print(f\"t1: {t1}\")\n",
    "        print(f\"t2: {t2}\")\n",
    "        \"\"\"\n",
    "        t_dist = t2 - t1\n",
    "\n",
    "        multiplier = jnp.exp(self.gamma(k) ** 2) / (self.true_d[j] + self.true_d[k])\n",
    "\n",
    "        first_multiplier = jnp.exp(-self.true_d[k] * t_dist)\n",
    "        first_erf_terms = jax.scipy.special.erf(\n",
    "            (t_dist / self.l) - self.gamma(k)\n",
    "        ) + jax.scipy.special.erf(t1 / self.l + self.gamma(k))\n",
    "\n",
    "        second_multiplier = jnp.exp(-(self.true_d[k] * t2 + self.true_d[j] * t1))\n",
    "        second_erf_terms = jax.scipy.special.erf(\n",
    "            (t2 / self.l) - self.gamma(k)\n",
    "        ) + jax.scipy.special.erf(self.gamma(k))\n",
    "\n",
    "        result = multiplier * (\n",
    "            jnp.multiply(first_multiplier, first_erf_terms)\n",
    "            - jnp.multiply(second_multiplier, second_erf_terms)\n",
    "        )\n",
    "\n",
    "        #print(f\"result: {result}\")\n",
    "        return result\n",
    "\n",
    "    def gamma(self, k: Int[Array, \" O\"]) -> ScalarFloat:\n",
    "        # Gamma term for h function\n",
    "        return (self.true_d[k] * self.l) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom mean function\n",
    "\n",
    "The mean function for the gene expressions per time is used to obtain the basal rates:\n",
    "\n",
    "$$\n",
    "f(x_{j}) = \\frac{B_{j}}{D_{j}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom mean\n",
    "@dataclass\n",
    "class latent_mean(gpx.mean_functions.AbstractMeanFunction):\n",
    "    r\"\"\"\n",
    "    Simple Input Motif mean function\n",
    "\n",
    "    ```math\n",
    "        f(x_{j}) = \\frac{B_{j}}{D_{j}} \n",
    "    ```\n",
    "\n",
    "    From equation 2 in paper.\n",
    "\n",
    "    $B_{j}$ represents the basal rate for gene $j$ and is a trainable paramater.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pass kernel instance that contains true_d parameter\n",
    "    kernel: gpx.kernels.AbstractKernel = field(default_factory=lambda: latent_kernel())\n",
    "    \n",
    "    #true_d: Float[Array, \"1 5\"] = static_field(jnp.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=jnp.float64))\n",
    "    \n",
    "    # Define parameters\n",
    "    initial_constrained_b = jnp.array([0.05, 0.05, 0.05, 0.05, 0.05], dtype=jnp.float64)\n",
    "    \n",
    "    true_b: Float[Array, \"1 5\"] = param_field(initial_constrained_b, bijector=tfb.Softplus(), metadata={\"name\": \" basal_rates\"}, trainable=True,)\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"N D\"]) -> Float[Array, \"N O\"]:\n",
    "        r\"\"\"Evaluate the mean function at the given points.\n",
    "\n",
    "        Args:\n",
    "            x (Float[Array, \" D\"]): The point at which to evaluate the mean function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Float[Array, \"1\"]: The evaluated mean function.\n",
    "        \"\"\"\n",
    "        # BUG: Concretization error if this is used ?\n",
    "        #num_genes = len(jnp.unique(x[:, 1]))\n",
    "        num_genes = 5\n",
    "\n",
    "        decays = self.kernel.true_d\n",
    "        \n",
    "        block_size = (x.shape[0] // num_genes)\n",
    "        # Define the mean function\n",
    "        #mean = (self.true_b / self.true_d).reshape(1, -1)\n",
    "        mean = (self.true_b / decays).reshape(1, -1)\n",
    "        mean = mean.repeat(block_size, jnp.newaxis).reshape(-1,1)\n",
    "\n",
    "        return mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = gpx.Dataset(training_times, gene_expressions)\n",
    "\n",
    "testing_times = generate_test_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_mll(posterior, dataset, NIters=1000, key=key):\n",
    "    # define the MLL using dataset_train\n",
    "    objective = gpx.objectives.ConjugateMLL(negative=True)\n",
    "    print(f'MLL before opt: {objective(posterior, dataset):.3f}')\n",
    "    # Optimise to minimise the MLL\n",
    "    opt_posterior, history = gpx.fit_scipy(\n",
    "        model=posterior,\n",
    "        objective=objective,\n",
    "        train_data=dataset,\n",
    "    )\n",
    "    return opt_posterior, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_function = gpx.mean_functions.Zero()\n",
    "kernel = latent_kernel()\n",
    "\n",
    "# Define the model\n",
    "prior = gpx.gps.Prior(mean_function = mean_function, kernel = kernel, jitter=1e-4)\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=dataset_train.n)\n",
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/project_wp289/lib/python3.11/site-packages/cola/backends/backends.py:75: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(cls, tree_flatten, tree_unflatten)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLL before opt: 44.145\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 8.756255\n",
      "         Iterations: 38\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 44\n"
     ]
    }
   ],
   "source": [
    "optimised_posterior, history = optimise_mll(posterior, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = optimised_posterior.predict(testing_times, train_data=dataset_train)\n",
    "predictive_dist = optimised_posterior.likelihood(latent_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5516667       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan       nan       nan       nan       nan\n",
      "       nan       nan       nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/project_wp289/lib/python3.11/site-packages/cola/backends/backends.py:75: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(cls, tree_flatten, tree_unflatten)\n"
     ]
    }
   ],
   "source": [
    "print(latent_dist.stddev())\n",
    "print(predictive_dist.stddev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom .predict method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_wp289",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
